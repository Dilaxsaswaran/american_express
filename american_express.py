# randomForest_american_express.ipynb

# Automatically generated by Colaboratory.

# Original file is located at
#     https://colab.research.google.com/drive/1P1Qo6BzJmysGXgxpG3DzL2QVIwzBr4Qh

# It is defined by the kaggle/python Docker image:
import numpy as np 
import pandas as pd 

# Input data files are available in the read-only "../input/" directory
import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
def convert_bytes(size):
    """ Convert bytes to KB, or MB or GB"""
    for x in ['bytes', 'KB', 'MB', 'GB', 'TB']:
        if size < 1024.0:
            return "%3.1f %s" % (size, x)
        size /= 1024.0

# display CSV file with size
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        csvfile=os.path.join(dirname, filename)
        csvfilesize = os.path.getsize(csvfile)
        filesize = convert_bytes(csvfilesize)
        print(f'{csvfile} size is', filesize, 'bytes')

# Importing the dataset
from pathlib import Path

input_path = Path('/kaggle/input/amex-default-prediction/')

# Loading dataset train_data.csv
import dask.dataframe as dd

# Loading dataset train_data.csv
train_df_sample = pd.read_csv('../input/amex-default-prediction/train_data.csv', nrows=100000)

# get shape of dataframe
print('Shape of dataset is:', train_df_sample.shape)

# print summary of dataframe
train_df_sample.info()

# Loading dataset train_labels.csv
train_label_df = pd.read_csv('../input/amex-default-prediction/train_labels.csv')

# get shape of dataframe
print('Shape of dataset is:', train_label_df.shape)

# print summary of dataframe
train_label_df.info()

# Loading dataset test_data.csv
test_df = pd.read_csv('../input/amex-default-prediction/test_data.csv', nrows=100000, index_col='customer_ID')

# get shape of dataframe
print('Shape of dataset is:', test_df.shape)

test_df.info()

# Merge train_df_sample and train_label_df
train_df = dd.merge(train_df_sample,train_label_df,on='customer_ID')

# print summary of merged dataframe
train_df.info()

# shape of train dataframe
train_df.shape

# Top 5 records of the data frame --> observe NaN values in the data frame
train_df.head()

# Shape of test dataframe
test_df.shape

# Top 5 records of test data frame
test_df.head()

#Check if there are null/missing values
train_df.isnull()

# Check if there are null/missing values
i=0
for col in train_df.columns:
    if (train_df[col].isnull().sum()/len(train_df[col])*100) >=70:
        print("Column Dropped", col)
        train_df.drop(labels=col,axis=1,inplace=True)
        i=i+1
        
print("Total dropped columns are", i)

# Check if there are null/missing values
i=0
for col in test_df.columns:
    if (test_df[col].isnull().sum()/len(test_df[col])*100) >=70:
        print("Column Dropped", col)
        test_df.drop(labels=col,axis=1,inplace=True)
        i=i+1
        
print("Total dropped columns are", i)

#  Check if there are null/missing values
def drop_features():
  train_df.drop(columns=['customer_id', 'S_2'], inplace=True)

# Check if there are null/missing values
def drop_features():
  test_df.drop(columns=['customer_id', 'S_2'], inplace=True)

# installation
!pip install dtale

# Importing Libraries
import dtale
d = dtale.show(train_df)
d = dtale.show(test_df)
d.open_browser()

#Installing Library
!pip install xverse

# Importing Libraries
import xverse  #run if required

# WOE Transformation

X = train_df.drop('target',axis=1)
y = train_df['target']
from xverse.transformer import WOE
clf = WOE()
clf.fit(X, y)

#Applying Tranformation
X = clf.transform(X)
X

#RANDOM FOREST CLASSIFIER
import warnings
warnings.filterwarnings('ignore')
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import GradientBoostingClassifier,RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix,classification_report
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size= .20,random_state=20)
clf = WOE()
clf.fit(Xtrain, ytrain)
Xtrain = clf.transform(Xtrain)
Xtest = clf.transform(Xtest)
rf = RandomForestClassifier(n_estimators=100,class_weight='balanced')
rf.fit(Xtrain,ytrain)
print("Training Accuracy")
print(rf.score(Xtrain,ytrain))
print("Testing Accuracy")
print(rf.score(Xtest,ytest))
predicted = rf.predict(Xtest)
print(confusion_matrix(ytest,predicted))
print(classification_report(ytest,predicted))

# cross validation
scoresdt = cross_val_score(rf,Xtrain,ytrain,cv=10,scoring='f1')
print(scoresdt)
print("Average f1")
print(np.mean(scoresdt))

#liner regression prediction
rf.predict(Xtest)

#liner regression predict proba
rf.predict_proba(Xtest)

# Predicting the Test set results
prediction = rf.predict_proba(Xtest)
final_predictions = prediction[:,1]

#submission file
output = pd.DataFrame({'customer_ID': Xtest.index,'prediction': ytest})
output.to_csv('submission.csv', index=False, header=True)
